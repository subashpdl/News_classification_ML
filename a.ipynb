{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3xPiZS5KL6QM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import (LSTM, Conv1D, Dense, Embedding, Flatten,\n",
    "                          GlobalMaxPooling1D, Input, MaxPooling1D)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, filename=\"Default.log\"):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, \"w\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        self.log.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()\n",
    "\n",
    "\n",
    "sys.stdout = Logger(\"eval.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYx7gwzSnBUh"
   },
   "source": [
    "## Preprocessing\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "PRuT9YXXMJbk",
    "outputId": "d1eed9b6-771d-49df-8c5d-e9e89438b0c0"
   },
   "outputs": [],
   "source": [
    "GLOVE_FILE = \"glove.6B.100d.txt\"\n",
    "DATA_FILE = \"twentyng_processed.json\"\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 30000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "print('Loading word vectors...')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE,  encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print(\"Processing text dataset\")\n",
    "\n",
    "with open(DATA_FILE, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts_cleaned = [d[\"cleaned\"] for d in data[\"data\"]]  # list of text samples\n",
    "texts_cleaned_stemmed = [d[\"cleaned_stemmed\"] for d in data[\"data\"]] # list of text samples\n",
    "\n",
    "# dictionary mapping label name to numeric id\n",
    "label_id_to_str = data[\"label_id_to_str\"]\n",
    "# list of labels for each text in texts as an id\n",
    "labels_id = [d[\"label_id\"] for d in data[\"data\"]]\n",
    "# list of labels for each text in texts as an id\n",
    "labels_str = [d[\"label_str\"] for d in data[\"data\"]]\n",
    "\n",
    "print(\"Loaded %s texts.\" % len(texts_cleaned))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "\"\"\"\n",
    "Keras provides a Tokenizer class that can be fit on the training data, can convert text to sequences consistently by \n",
    "calling the texts_to_sequences() method on the Tokenizer class, and provides access to the dictionary \n",
    "mapping of words to integers in a word_index attribute.\n",
    "\"\"\"\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts_cleaned)\n",
    "sequences = tokenizer.texts_to_sequences(texts_cleaned)\n",
    "# max sequence length is the maximum number of words in a blog post\n",
    "texts_cleaned_padded = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# input are vectors [] length max_sequence_length with [id, id, id, id, pad, pad, pad]\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "type(texts_cleaned_padded)\n",
    "type(word_index)\n",
    "# one hot encode the output variable\n",
    "labels_id = to_categorical(np.asarray(labels_id))\n",
    "print('Shape of texts tensor:', texts_cleaned_padded.shape)\n",
    "print('Shape of label tensor:', labels_id.shape)\n",
    "#print(texts_cleaned_stemmed[:5])\n",
    "#print(labels_id)\n",
    "#print(\"String\", labels_str)\n",
    "#print(\"label id to str\", label_id_to_str)\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "embeddings_index_reduced = {}\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        embeddings_index_reduced[word] = embedding_vector\n",
    "\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "train_texts_stemmed, test_texts_stemmed, train_texts_cleaned, test_texts_cleaned, train_labels_id, test_labels_id, train_labels_str, test_labels_str = train_test_split(\n",
    "    texts_cleaned_stemmed, texts_cleaned_padded, labels_id, labels_str, test_size=VALIDATION_SPLIT, random_state=42, stratify=labels_str)\n",
    "\n",
    "print(\"Generating embedding matrices\")\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer_globve = Embedding(num_words,\n",
    "                                   EMBEDDING_DIM,\n",
    "                                   weights=[embedding_matrix],\n",
    "                                   input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                   trainable=False)\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "\n",
    "def get_embedding_layer_globve_trainable():\n",
    "  em = np.copy(embedding_matrix)\n",
    "  return Embedding(num_words,EMBEDDING_DIM,weights=[em],input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
    "   \n",
    "\n",
    "def get_embedding_layer_empty():\n",
    "  return Embedding(len(word_index) + 1,EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--oF8Jj4nKKm"
   },
   "source": [
    "## Naive Bayes\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "btqWQCWkMfA_",
    "outputId": "70028de2-69c5-44b7-f9e1-b10a8ecafcba"
   },
   "outputs": [],
   "source": [
    "print('Training baseline model.')\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer=\"word\",\n",
    "                                              tokenizer=None,  # irrelevant, we do this manually beforehand\n",
    "                                              preprocessor=None,  # irrelevant, we do this manually beforehand\n",
    "                                              lowercase=True,  # irrelevant, we already do this\n",
    "                                              max_features=MAX_NUM_WORDS,  # limit dictionary\n",
    "                                              )),\n",
    "                     # found by grid search\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                     # found by grid search\n",
    "                     ('clf', MultinomialNB(alpha=0.015, fit_prior=False)),\n",
    "                     ])\n",
    "\n",
    "print(\"NB BASE LINE\")\n",
    "text_clf = text_clf.fit(train_texts_stemmed, train_labels_str)\n",
    "predicted = text_clf.predict(test_texts_stemmed)\n",
    "print(metrics.classification_report(test_labels_str, predicted))\n",
    "print(np.mean(predicted == test_labels_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLZ-3xFvKLDL"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "id": "yroI1MmKJGvN",
    "outputId": "f7f38ee0-f77c-400c-88ce-d4eb9d340747"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                  ])\n",
    "\n",
    "logreg.fit(train_texts_stemmed, train_labels_str)\n",
    "\n",
    "predicted = logreg.predict(test_texts_stemmed)\n",
    "\n",
    "print(metrics.classification_report(test_labels_str, predicted))\n",
    "print('accuracy %s' % accuracy_score(test_labels_str, predicted))\n",
    "#print(classification_report(test_labels_str, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEwOCJp5WFLX"
   },
   "source": [
    "## Linear SVM\n",
    "\n",
    "---\n",
    "SGD uses only a single sample, i.e., a batch size of one, to perform each iteration. The sample is randomly shuffled and selected for performing the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "R31KpfuxLZHv",
    "outputId": "77acc5a4-3f39-4b4c-873f-7e2b6e24e5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7116710875331564\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.72      0.51      0.60       160\n",
      "           comp.graphics       0.71      0.66      0.68       195\n",
      " comp.os.ms-windows.misc       0.66      0.64      0.65       197\n",
      "comp.sys.ibm.pc.hardware       0.71      0.69      0.70       196\n",
      "   comp.sys.mac.hardware       0.83      0.75      0.79       193\n",
      "          comp.windows.x       0.80      0.77      0.78       198\n",
      "            misc.forsale       0.76      0.77      0.76       195\n",
      "               rec.autos       0.73      0.75      0.74       198\n",
      "         rec.motorcycles       0.48      0.72      0.57       199\n",
      "      rec.sport.baseball       0.78      0.76      0.77       199\n",
      "        rec.sport.hockey       0.77      0.90      0.83       200\n",
      "               sci.crypt       0.74      0.76      0.75       198\n",
      "         sci.electronics       0.70      0.50      0.58       197\n",
      "                 sci.med       0.75      0.88      0.81       198\n",
      "               sci.space       0.78      0.81      0.80       197\n",
      "  soc.religion.christian       0.61      0.85      0.71       199\n",
      "      talk.politics.guns       0.66      0.77      0.71       182\n",
      "   talk.politics.mideast       0.78      0.86      0.82       188\n",
      "      talk.politics.misc       0.72      0.44      0.55       155\n",
      "      talk.religion.misc       0.64      0.14      0.23       126\n",
      "\n",
      "                accuracy                           0.71      3770\n",
      "               macro avg       0.72      0.70      0.69      3770\n",
      "            weighted avg       0.72      0.71      0.70      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "\n",
    "sgd.fit(train_texts_stemmed, train_labels_str)\n",
    "predicted = sgd.predict(test_texts_stemmed)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(test_labels_str, predicted))\n",
    "\n",
    "print(metrics.classification_report(test_labels_str, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZprNHCbihKhi"
   },
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "n7RXohHqc01S",
    "outputId": "29f86f19-e86d-4200-9429-cf62ac9ce8c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:00<00:00, 3327966.20it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3441564.49it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3459942.80it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3775234.18it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3615012.04it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3901187.11it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3941846.77it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3932629.51it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3823442.64it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3706200.92it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3525370.31it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3448019.77it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3309296.37it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3453442.84it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3602819.20it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3484805.94it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3196225.51it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 2699284.70it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 2679338.80it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 2852816.99it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 2791167.13it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3203998.75it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3155145.22it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3287823.52it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3282226.18it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3210505.39it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3212723.67it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3157792.15it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3260564.01it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3290697.86it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 3305559.87it/s]\n"
     ]
    }
   ],
   "source": [
    "######Doc2Vec######\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "#!pip install gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "import re\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v, [label]))\n",
    "    return labeled\n",
    "\n",
    "x_train = label_sentences(train_texts_stemmed, 'Train')\n",
    "x_test = label_sentences(test_texts_stemmed, 'Test')\n",
    "\n",
    "all_data = x_train + x_test\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "R2bWzsRB7wU2",
    "outputId": "58f70d5b-2c5e-4f33-da4e-d9bed0911a77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-60e246afbb9b>:16: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  vectors[i] = model.docvecs[prefix]\n",
      "/Users/abderrahmanecharrade/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.18143236074270558\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.13      0.09      0.11       160\n",
      "           comp.graphics       0.16      0.14      0.15       195\n",
      " comp.os.ms-windows.misc       0.24      0.25      0.25       197\n",
      "comp.sys.ibm.pc.hardware       0.19      0.20      0.19       196\n",
      "   comp.sys.mac.hardware       0.17      0.13      0.15       193\n",
      "          comp.windows.x       0.28      0.51      0.36       198\n",
      "            misc.forsale       0.20      0.20      0.20       195\n",
      "               rec.autos       0.09      0.11      0.10       198\n",
      "         rec.motorcycles       0.13      0.15      0.14       199\n",
      "      rec.sport.baseball       0.21      0.22      0.22       199\n",
      "        rec.sport.hockey       0.18      0.23      0.20       200\n",
      "               sci.crypt       0.19      0.24      0.21       198\n",
      "         sci.electronics       0.11      0.07      0.09       197\n",
      "                 sci.med       0.20      0.15      0.17       198\n",
      "               sci.space       0.11      0.09      0.10       197\n",
      "  soc.religion.christian       0.19      0.25      0.22       199\n",
      "      talk.politics.guns       0.11      0.07      0.08       182\n",
      "   talk.politics.mideast       0.27      0.37      0.31       188\n",
      "      talk.politics.misc       0.08      0.03      0.04       155\n",
      "      talk.religion.misc       0.05      0.03      0.04       126\n",
      "\n",
      "                accuracy                           0.18      3770\n",
      "               macro avg       0.16      0.18      0.17      3770\n",
      "            weighted avg       0.17      0.18      0.17      3770\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abderrahmanecharrade/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(x_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(x_test), 300, 'Test')\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, train_labels_str)\n",
    "logreg = logreg.fit(train_vectors_dbow, train_labels_str)\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test_labels_str))\n",
    "print(metrics.classification_report(test_labels_str, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qdxyuPz6ip8O"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history, desc):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy\\n' + desc)\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss\\n' + desc)\n",
    "    plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "N2osu0MRL9aU"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "def evaluateModel(model, description, epochs, batch_size, train_texts, train_labels_id, test_texts, test_labels_id):\n",
    "    try:\n",
    "        print(\"\\n\\n#################### %s ####################\" % (description))\n",
    "        print(model.summary())\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        csv_logger = CSVLogger('/models/' + description + '_run_log.csv')\n",
    "        history = model.fit(train_texts, train_labels_id, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(test_texts, test_labels_id), callbacks=[csv_logger])\n",
    "        #print(\"\\n final results on test set:\")\n",
    "        model.evaluate(test_texts, test_labels_id)\n",
    "        plot_history(history, description)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\ncanceled learning this network!\")\n",
    "    return history\n",
    "\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raLupjzrpKZx"
   },
   "source": [
    "## LSTM + GlobVe Embedding Retrained\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1784
    },
    "id": "mTW6LsoqlEkV",
    "outputId": "83ac2f39-78de-4fc4-878f-c760e0666b11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#################### LSTM + GlobVe Embedding Retrained ####################\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          3000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 3,119,828\n",
      "Trainable params: 3,119,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/models/LSTM + GlobVe Embedding Retrained_run_log.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b2553a1aaee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_id_to_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m evaluateModel(model, \"LSTM + GlobVe Embedding Retrained\", EPOCHS, BATCH_SIZE,\n\u001b[0m\u001b[1;32m      8\u001b[0m               train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n",
      "\u001b[0;32m<ipython-input-22-c2e6e3fb5f9b>\u001b[0m in \u001b[0;36mevaluateModel\u001b[0;34m(model, description, epochs, batch_size, train_texts, train_labels_id, test_texts, test_labels_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcsv_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_run_log.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(\"\\n final results on test set:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1136\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m       \u001b[0;31m# Handle fault-tolerance for multi-worker.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2741\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2743\u001b[0;31m     self.csv_file = io.open(self.filename,\n\u001b[0m\u001b[1;32m   2744\u001b[0m                             \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m                             **self._open_args)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/models/LSTM + GlobVe Embedding Retrained_run_log.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "########################\n",
    "model = Sequential()\n",
    "model.add(get_embedding_layer_globve_trainable())\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"LSTM + GlobVe Embedding Retrained\", EPOCHS, BATCH_SIZE,\n",
    "              train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RONfRuVGpmFy"
   },
   "source": [
    "## LSTM + GlobVe Embedding Retrained Batch and Cells\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20165
    },
    "id": "IartuENQSG4k",
    "outputId": "4a082c00-ebfc-4b0f-9e8e-0ea5c8e4235a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#################### LSTM + GlobVe Embedding Retrained Batch 128 Cells 64 ####################\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          3000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 3,043,540\n",
      "Trainable params: 3,043,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/models/LSTM + GlobVe Embedding Retrained Batch 128 Cells 64_run_log.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c4346113747d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_id_to_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mevaluateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LSTM + GlobVe Embedding Retrained Batch %i Cells %i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miCells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_texts_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_texts_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-c2e6e3fb5f9b>\u001b[0m in \u001b[0;36mevaluateModel\u001b[0;34m(model, description, epochs, batch_size, train_texts, train_labels_id, test_texts, test_labels_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcsv_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_run_log.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(\"\\n final results on test set:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1136\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m       \u001b[0;31m# Handle fault-tolerance for multi-worker.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2741\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2743\u001b[0;31m     self.csv_file = io.open(self.filename,\n\u001b[0m\u001b[1;32m   2744\u001b[0m                             \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m                             **self._open_args)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/models/LSTM + GlobVe Embedding Retrained Batch 128 Cells 64_run_log.csv'"
     ]
    }
   ],
   "source": [
    "########################\n",
    "for iBATCH_SIZE in [128, 256, 512]:\n",
    "  for iCells in [64, 128, 256, 512]:\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer_globve_trainable())\n",
    "    model.add(LSTM(iCells, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "    evaluateModel(model, \"LSTM + GlobVe Embedding Retrained Batch %i Cells %i\" % (iBATCH_SIZE, iCells), EPOCHS, iBATCH_SIZE, train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXG7yG-WN5QI"
   },
   "source": [
    "## simple dense layer neuronal network\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1747
    },
    "id": "W5zLoFKfkgha",
    "outputId": "eeaccc14-3d9e-49aa-c073-1f9579052a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#################### simple dense layer neuronal network ####################\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          3000000   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100, 256)          25856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                512020    \n",
      "=================================================================\n",
      "Total params: 3,537,876\n",
      "Trainable params: 537,876\n",
      "Non-trainable params: 3,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/models/simple dense layer neuronal network_run_log.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7964c3eb62b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_id_to_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mevaluateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"simple dense layer neuronal network\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_texts_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_texts_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-c2e6e3fb5f9b>\u001b[0m in \u001b[0;36mevaluateModel\u001b[0;34m(model, description, epochs, batch_size, train_texts, train_labels_id, test_texts, test_labels_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcsv_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_run_log.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(\"\\n final results on test set:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1136\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m       \u001b[0;31m# Handle fault-tolerance for multi-worker.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2741\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2743\u001b[0;31m     self.csv_file = io.open(self.filename,\n\u001b[0m\u001b[1;32m   2744\u001b[0m                             \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m                             **self._open_args)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/models/simple dense layer neuronal network_run_log.csv'"
     ]
    }
   ],
   "source": [
    "###############\n",
    "model = Sequential()\n",
    "model.add(embedding_layer_globve)\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "evaluateModel(model, \"simple dense layer neuronal network\", EPOCHS, BATCH_SIZE, train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHOZYdXQOl0A"
   },
   "source": [
    "## simple dense layer neuronal network retrainable Dense\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6869
    },
    "id": "lWSUGjpPHkX3",
    "outputId": "04aa5ec0-d789-47be-b9a3-89446ca3c1de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#################### simple dense layer neuronal network retrainable Dense 128 ####################\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 100)          3000000   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100, 128)          12928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                256020    \n",
      "=================================================================\n",
      "Total params: 3,268,948\n",
      "Trainable params: 3,268,948\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/models/simple dense layer neuronal network retrainable Dense 128_run_log.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6a2b37bb074d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_id_to_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mevaluateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"simple dense layer neuronal network retrainable Dense %i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_texts_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_texts_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-c2e6e3fb5f9b>\u001b[0m in \u001b[0;36mevaluateModel\u001b[0;34m(model, description, epochs, batch_size, train_texts, train_labels_id, test_texts, test_labels_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcsv_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_run_log.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(\"\\n final results on test set:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1136\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m       \u001b[0;31m# Handle fault-tolerance for multi-worker.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2741\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2743\u001b[0;31m     self.csv_file = io.open(self.filename,\n\u001b[0m\u001b[1;32m   2744\u001b[0m                             \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m                             **self._open_args)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/models/simple dense layer neuronal network retrainable Dense 128_run_log.csv'"
     ]
    }
   ],
   "source": [
    "###############\n",
    "for iDense in [128, 256, 512, 1024]:\n",
    "  model = Sequential()\n",
    "  model.add(get_embedding_layer_globve_trainable())\n",
    "  model.add(Dense(iDense, activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "  evaluateModel(model, \"simple dense layer neuronal network retrainable Dense %i\" % iDense, EPOCHS, BATCH_SIZE, train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBlggtT7RiDO"
   },
   "source": [
    "## Double dense layer neuronal network retrainable Dense\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5258
    },
    "id": "i5Xn7T0zHxme",
    "outputId": "f9c541b8-592f-4d84-c033-3ecbf33022cb"
   },
   "outputs": [],
   "source": [
    "###############\n",
    "for iDense in [128, 256, 512]:\n",
    "  model = Sequential()\n",
    "  model.add(get_embedding_layer_globve_trainable())\n",
    "  model.add(Dense(iDense, activation='relu'))\n",
    "  model.add(Dense(iDense, activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "  evaluateModel(model, \"double dense layer neuronal network retrainable Dense %i\" % iDense, EPOCHS, BATCH_SIZE, train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGIJ4B-UTrCb"
   },
   "source": [
    "## cnn + lstm + GlobVe Embedding\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1781
    },
    "id": "k8johSHqjve-",
    "outputId": "a2f3668b-8c21-4a4b-a96c-47ed7caf466e"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "model = Sequential()\n",
    "model.add(embedding_layer_globve)\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"cnn + lstm + GlobVe Embedding\", EPOCHS, BATCH_SIZE,\n",
    "              train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9MVg1TedUvQ"
   },
   "source": [
    "## simple one layer neuronal network retrainable\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1781
    },
    "id": "VCPxAThvmihF",
    "outputId": "0ce78ac4-1749-4cfc-ec98-1311751483b6"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "model = Sequential()\n",
    "model.add(get_embedding_layer_globve_trainable())\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"cnn + lstm + GlobVe Embedding Retrainable\", EPOCHS, BATCH_SIZE,\n",
    "              train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK58c0v0XY8O"
   },
   "source": [
    "## simple one layer neuronal network\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1713
    },
    "id": "8d-5uXNqkj3S",
    "outputId": "ed7381e2-560c-4a78-a1f6-e63b232b5fcd"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############\n",
    "model = Sequential()\n",
    "model.add(embedding_layer_globve)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"simple one layer neuronal network\", EPOCHS, BATCH_SIZE,\n",
    "              train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djB770q0dMOy"
   },
   "source": [
    "## simple one layer neuronal network retrainable\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1713
    },
    "id": "8LDAknH8mvaX",
    "outputId": "2a5e4a5a-1cad-4b5a-9fa1-86dc33c4a10e"
   },
   "outputs": [],
   "source": [
    "###############\n",
    "model = Sequential()\n",
    "model.add(get_embedding_layer_globve_trainable())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"simple one layer neuronal network retrainable\", EPOCHS, BATCH_SIZE,\n",
    "              train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOkWhX1AYEqG"
   },
   "source": [
    "## LSTM without dropout + custom embedding\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1713
    },
    "id": "Kb37JLYWklss",
    "outputId": "5bfb22a3-beb9-407a-ab93-9949f4ea40a4"
   },
   "outputs": [],
   "source": [
    "\n",
    "########################\n",
    "model = Sequential()\n",
    "model.add(get_embedding_layer_empty())\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"LSTM without dropout + custom embedding\", EPOCHS, BATCH_SIZE,\n",
    "              train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzLmH9iUY6u4"
   },
   "source": [
    "## LSTM + custom Embedding\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1713
    },
    "id": "oIUW845Tkmqp",
    "outputId": "037177e6-6e5a-41dc-877c-ffbb9da88160"
   },
   "outputs": [],
   "source": [
    "\n",
    "########################\n",
    "model = Sequential()\n",
    "model.add(get_embedding_layer_empty())\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"LSTM + custom Embedding\", EPOCHS, BATCH_SIZE,\n",
    "              train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDLppo7xZCp7"
   },
   "source": [
    "## LSTM + GlobVe Embedding\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1713
    },
    "id": "FYDZ64wTknrG",
    "outputId": "e411e35b-f377-4ffe-d3d1-ae6284d34730"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "########################\n",
    "model = Sequential()\n",
    "model.add(embedding_layer_globve)\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"LSTM + GlobVe Embedding\", EPOCHS, BATCH_SIZE,\n",
    "              train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VP6c9lNdC7o"
   },
   "source": [
    "## stacked lstm + GlobVe Embedding\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1747
    },
    "id": "zSE5BVd4kojK",
    "outputId": "12a0b820-7a0d-4ebb-db86-fe1704395b88"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "model = Sequential()\n",
    "model.add(embedding_layer_globve)\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"stacked lstm + GlobVe Embedding\", EPOCHS, BATCH_SIZE, train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26sHE3SFc65r"
   },
   "source": [
    "## stacked lstm + GlobVe Embedding Retrainable\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1747
    },
    "id": "t__ZTsf4gg4H",
    "outputId": "60942eb8-f685-4406-efec-3ae2b0c0adfd"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "model = Sequential()\n",
    "model.add(get_embedding_layer_globve_trainable())\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"stacked lstm + GlobVe Embedding Retrainable\", EPOCHS, BATCH_SIZE, train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyXoifEmaHTA"
   },
   "source": [
    "## Conv1d with retrained embedding\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1988
    },
    "id": "LB1Gkq73RaLg",
    "outputId": "7b06cf34-f4bf-4989-87e9-31a1adb77b4d"
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################\n",
    "# train a 1D convnet with global maxpooling\n",
    "model = Sequential()\n",
    "model.add(get_embedding_layer_globve_trainable())\n",
    "model.add(Conv1D(128, 4, activation='relu'))\n",
    "# max pooling ist ein subsampling mit max(), schneller, genauer\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(Conv1D(128, 4, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# this is the output layer. it has as many neurons as we have labels\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"Conv1d with retrained embedding\", EPOCHS, BATCH_SIZE, train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS1Qp9wkcz_f"
   },
   "source": [
    "## simple dense layer neuronal network\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1747
    },
    "id": "hxRv_4VtNAYX",
    "outputId": "546e5173-4f95-45aa-aa26-8e86f39d1d84"
   },
   "outputs": [],
   "source": [
    "###############\n",
    "#####Summation#\n",
    "model = Sequential()\n",
    "model.add(embedding_layer_globve)\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "evaluateModel(model, \"simple dense layer neuronal network\", EPOCHS, BATCH_SIZE, train_texts_cleaned, train_labels_id, test_texts_cleaned, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGmr3a7Hcj9A"
   },
   "source": [
    "## BOW with dense 1024\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1679
    },
    "id": "VHx9kspKkqaG",
    "outputId": "abc8d39d-2bca-45dd-83dd-c21b064047a6"
   },
   "outputs": [],
   "source": [
    "###############\n",
    "###############\n",
    "#BAG OF WORDS##\n",
    "from keras.layers import Activation, Dense\n",
    "# from https://cloud.google.com/blog/products/gcp/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts\n",
    "\n",
    "train_texts_stemmed_bow = tokenizer.texts_to_matrix(train_texts_stemmed)\n",
    "test_texts_stemmed_bow = tokenizer.texts_to_matrix(test_texts_stemmed)\n",
    "\n",
    "# labels_id one hot encoded output\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(MAX_NUM_WORDS,), activation='relu'))\n",
    "model.add(Dense(len(label_id_to_str), activation='softmax'))\n",
    "\n",
    "evaluateModel(model, \"bag of words dense 1024\", EPOCHS, BATCH_SIZE, train_texts_stemmed_bow, train_labels_id, test_texts_stemmed_bow, test_labels_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfeHVnp6F-2g"
   },
   "source": [
    "## HAN\n",
    "\n",
    "---\n",
    " It uses stacked recurrent neural networks on word level followed by attention model to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. Then the same procedure applied to the derived sentence vectors which then generate a vector who conceives the meaning of the given document and that vector can be passed further for text classification.\n",
    " The idea behind the paper is “Words make sentences and sentences make documents”. The intent is to derive sentence meaning from the words and then derive the meaning of the document from those sentences. But not all words are equally important. Some of them characterize a sentence more than others. Therefore we use the attention model so that sentence vector can have more attention on “important” words. Attention model consists of two parts: Bidirectional RNN and Attention networks. \n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*28XVtq2lOjOmZhcSgu1NmQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Jl6DC44GKvnc",
    "outputId": "5e0eeaab-bb6f-4f44-f1fd-90dd0ee5d0c1"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zl4fHVdCcrj"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "MAX_SEQ = 15\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttentionLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SEQ, MAX_SEQUENCE_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttentionLayer(100)(l_lstm_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "evaluateModel(model, \"Hierarchical Attention Networt\", EPOCHS, BATCH_SIZE, train_texts_stemmed_bow, train_labels_id, test_texts_stemmed_bow, test_labels_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rtIkBz-xWC4"
   },
   "source": [
    "## GridSearchCV\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDHoGVluxUyD"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(MAX_NUM_WORDS,), kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "np.random.seed(47)\n",
    "model = KerasClassifier(build_fn = create_model, verbose = 0)\n",
    "\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87fdKUV8FwVr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20ng_eval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
